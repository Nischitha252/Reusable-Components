{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb8ee95",
   "metadata": {},
   "source": [
    "## Large Document Processing with Azure OpenAI and FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ee5bf2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78132262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import fitz  # PyMuPDF\n",
    "import openai\n",
    "import os\n",
    "import pickle\n",
    "import glob  \n",
    "import pickle  \n",
    "import faiss  \n",
    "import numpy as np \n",
    "from typing import List, Tuple ,Optional\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d299501",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33660663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Azure embeddings and OpenAI client using environment variables\n",
    "load_dotenv() \n",
    "\n",
    "openai.api_base    = os.environ.get(\"#your_azure_openai_endpoint#\")\n",
    "openai.api_key     = os.environ.get(\"#your_azure_openai_key#\")\n",
    "openai.api_version = os.environ.get(\"#your_azure_openai_version#\")\n",
    "\n",
    "EMBEDDING_MODEL = os.environ.get(\"#your_azure_embedding_model#\")\n",
    "LLM_MODEL       = os.environ.get(\"#your_azure_llm_model#\")\n",
    "\n",
    "# ─── Instantiate the embeddings client ────────────────────────────────────────\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=openai.api_base,\n",
    "    api_key=openai.api_key,\n",
    "    azure_deployment=EMBEDDING_MODEL,\n",
    "    openai_api_version=openai.api_version,\n",
    ")\n",
    "\n",
    "# 3. Initialize your LLM model (AzureChatOpenAI)\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=openai.api_base,\n",
    "    api_key=openai.api_key,\n",
    "    azure_deployment=LLM_MODEL,\n",
    "    openai_api_version=openai.api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b94972",
   "metadata": {},
   "source": [
    "### Function to extract blocks from the large document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9456d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#....Extracts text from a PDF file, capturing each text block’s content, font size, vertical position, and page number.....#\n",
    "\n",
    "def extract_blocks_with_fonts(pdf_path: str) -> List[Dict]:\n",
    "    doc = fitz.open(pdf_path)  # Open the PDF document using PyMuPDF\n",
    "    all_blocks = []  # Initialize a list to store all extracted text blocks\n",
    "\n",
    "    for page_num, page in enumerate(doc, start=1):  # Loop through each page in the PDF\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]  # Extract all blocks (text/images) from the page\n",
    "\n",
    "        for block in blocks:  # Loop through each block on the page\n",
    "            if block[\"type\"] != 0:\n",
    "                continue  # Skip non-text blocks (e.g., images)\n",
    "\n",
    "            text = \"\"  # Initialize a string to collect text from the block\n",
    "            max_font_size = 0  # Track the largest font size in the block\n",
    "\n",
    "            for line in block[\"lines\"]:  # Loop through each line in the block\n",
    "                for span in line[\"spans\"]:  # Loop through each span (continuous text with same style)\n",
    "                    text += span[\"text\"]  # Add the span's text to the block's text\n",
    "                    max_font_size = max(max_font_size, span[\"size\"])  # Update max font size if needed\n",
    "\n",
    "            if text.strip():  # If the block contains non-empty text\n",
    "                all_blocks.append({\n",
    "                    \"text\": text.strip(),  # Store the cleaned text\n",
    "                    \"font_size\": max_font_size,  # Store the largest font size found in the block\n",
    "                    \"y0\": block[\"bbox\"][1],  # Store the vertical position (top) of the block\n",
    "                    \"page\": page_num  # Store the page number\n",
    "                })\n",
    "\n",
    "    return all_blocks  # Return the list of all extracted text blocks with their metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c088062",
   "metadata": {},
   "source": [
    "### Function to detect headings in the blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc93410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.....Automatically identifies headings based on font size and groups subsequent text blocks under these headings......#\n",
    "\n",
    "def detect_headings(blocks: List[Dict], font_size_threshold: float = 2.0) -> List[Tuple[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Detects headings using font size and groups blocks under each heading.\n",
    "    A heading is a block that has a significantly larger font than the previous average.\n",
    "    \"\"\"\n",
    "    avg_font = sum(b[\"font_size\"] for b in blocks) / len(blocks)  # Calculate the average font size across all blocks\n",
    "    threshold = avg_font + font_size_threshold  # Set the threshold for heading detection\n",
    "\n",
    "    sections = []  # Initialize a list to store the detected sections\n",
    "    current_section = {\"heading\": \"Introduction\", \"content\": []}  # Start with a default section\n",
    "\n",
    "    for block in blocks:  # Loop through each block in the list\n",
    "        if block[\"font_size\"] >= threshold:  # If the block's font size is above the threshold, treat as heading\n",
    "            # Save previous section if it has content\n",
    "            if current_section[\"content\"]:\n",
    "                sections.append((current_section[\"heading\"], current_section[\"content\"]))  # Add previous section to list\n",
    "            # Start a new section with the current block's text as the heading\n",
    "            current_section = {\"heading\": block[\"text\"], \"content\": []}\n",
    "        else:\n",
    "            current_section[\"content\"].append(block[\"text\"])  # Add block text to the current section's content\n",
    "\n",
    "    # After looping, add the last section if it has content\n",
    "    if current_section[\"content\"]:\n",
    "        sections.append((current_section[\"heading\"], current_section[\"content\"]))\n",
    "\n",
    "    return sections  # Return the list of sections, each as a tuple (heading, content list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc25cbe",
   "metadata": {},
   "source": [
    "### Function to chunk sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0208e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.....Divides sections into smaller chunks (max ~32KB each), making them manageable for embedding and retrieval......#\n",
    "\n",
    "def chunk_sections(sections: List[Tuple[str, List[str]]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits sections into multiple chunks such that:\n",
    "    - Each chunk does not exceed 32KB.\n",
    "    - Each chunk ends at the boundary of a section.\n",
    "    - All sections are preserved without being split.\n",
    "    \"\"\"\n",
    "    \n",
    "    chunks = []                    # List to hold the final chunks\n",
    "    current_chunk = \"\"            # String to build the current chunk\n",
    "    current_len = 0               # Byte length of the current chunk\n",
    "\n",
    "    MAX_CHUNK_SIZE = 32 * 1024    # Set max chunk size to 32KB (32 * 1024 bytes)\n",
    "\n",
    "    for heading, content in sections:\n",
    "        # Construct the full text of the section with heading and content\n",
    "        section_text = f\"{heading}\\n\" + \"\\n\".join(content) + \"\\n\\n\"\n",
    "        \n",
    "        # Compute the byte length of the section (not character length)\n",
    "        section_len = len(section_text.encode(\"utf-8\"))\n",
    "\n",
    "        # If the section by itself exceeds the max chunk size,\n",
    "        # treat it as an individual chunk\n",
    "        if section_len > MAX_CHUNK_SIZE:\n",
    "            if current_chunk:  # If there's any content in current chunk, save it first\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = \"\"\n",
    "                current_len = 0\n",
    "            chunks.append(section_text)  # Add large section as its own chunk\n",
    "            continue  # Move to the next section\n",
    "\n",
    "        # If adding the section exceeds the chunk limit,\n",
    "        # finalize the current chunk and start a new one\n",
    "        if current_len + section_len > MAX_CHUNK_SIZE:\n",
    "            chunks.append(current_chunk)      # Save current chunk\n",
    "            current_chunk = section_text      # Start a new chunk with current section\n",
    "            current_len = section_len         # Reset length tracker\n",
    "        else:\n",
    "            # Add section to the current chunk\n",
    "            current_chunk += section_text\n",
    "            current_len += section_len\n",
    "\n",
    "    # After the loop, if there's any remaining chunk, add it to the list\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks  # Return the list of chunked strings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d2419",
   "metadata": {},
   "source": [
    "### Fuction to save chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b86a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.....Stores each chunk into separate text files for easy access and future use.......#\n",
    "\n",
    "def save_chunks(chunks: List[str], output_dir: str):\n",
    "    # Create the output directory if it doesn't already exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate through the list of chunks with their index (starting from 1)\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        # Construct the file path for the current chunk (e.g., \"chunk_1.txt\")\n",
    "        chunk_path = os.path.join(output_dir, f\"chunk_{i}.txt\")\n",
    "\n",
    "        # Open the file in write mode with UTF-8 encoding\n",
    "        with open(chunk_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            # Write the chunk content to the file\n",
    "            f.write(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d751c2b3",
   "metadata": {},
   "source": [
    "### Function to embed chunks in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0be4d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .....Generates embeddings (vector representations) for all document chunks in parallel......#\n",
    "\n",
    "def embed_chunks_parallel(\n",
    "    chunks: List[str],              # List of text chunks to be embedded\n",
    "    embeddings_model: Any,         # The embedding model instance with `embed_documents` method\n",
    "    max_workers: int = None        # Optional: Max number of parallel threads (defaults to CPU count)\n",
    ") -> Dict[int, List[float]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings for each chunk in parallel.\n",
    "    Returns a dict {chunk_index: embedding_vector}.\n",
    "    \"\"\"\n",
    "\n",
    "    # Internal helper function to generate embedding for a single chunk\n",
    "    def _embed(idx: int, text: str):\n",
    "        emb = embeddings_model.embed_documents([text])[0]  # Generate embedding; output is a list with one item\n",
    "        return idx, emb  # Return the chunk index and the corresponding embedding\n",
    "\n",
    "    embeddings: Dict[int, List[float]] = {}  # Dictionary to store embeddings with chunk index as key\n",
    "\n",
    "    # Create a thread pool executor to run embedding tasks in parallel\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all embedding tasks to the executor\n",
    "        futures = {\n",
    "            executor.submit(_embed, i + 1, chunk): i + 1  # Key: future object, Value: chunk index\n",
    "            for i, chunk in enumerate(chunks)             # Enumerate chunks, starting index from 1\n",
    "        }\n",
    "\n",
    "        # Collect results as tasks complete\n",
    "        for future in as_completed(futures):\n",
    "            idx = futures[future]  # Get the corresponding chunk index for the future\n",
    "            try:\n",
    "                _, emb = future.result()  # Get the result (index and embedding) from the completed future\n",
    "                embeddings[idx] = emb     # Store the embedding in the dictionary\n",
    "                print(f\"[chunk_{idx}] Embedding generated.\")  # Log success\n",
    "            except Exception as e:\n",
    "                print(f\"[chunk_{idx}] Error generating embedding: {e}\")  # Log any error during embedding\n",
    "\n",
    "    return embeddings  # Return the dictionary of chunk index → embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daa9541",
   "metadata": {},
   "source": [
    "### Function to save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a434701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.......Stores these generated embeddings to system for future use.......\n",
    "\n",
    "def save_embeddings(\n",
    "    embeddings: Dict[int, Any],     # Dictionary mapping chunk index to its embedding\n",
    "    output_dir: str                 # Directory where embeddings will be saved\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save embeddings to disk under:\n",
    "        <output_dir>/embeddings/chunk_<idx>.pkl\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct the path to the 'embeddings' subdirectory inside the output directory\n",
    "    embeddings_dir = os.path.join(output_dir)\n",
    "    \n",
    "    # Create the directory if it doesn't already exist\n",
    "    os.makedirs(embeddings_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate through the dictionary of embeddings\n",
    "    for idx, emb in embeddings.items():\n",
    "        # Build the file path for the current chunk embedding file\n",
    "        file_path = os.path.join(embeddings_dir, f\"chunk_{idx}.pkl\")\n",
    "        \n",
    "        # Open the file in binary write mode and save the embedding using pickle\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(emb, f)\n",
    "        \n",
    "        # Print confirmation that the embedding was saved successfully\n",
    "        print(f\"[chunk_{idx}] Embedding saved to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25883057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1. Load chunks & embeddings in parallel ────────────────────\n",
    "def _load_one_pair(txt_path: str, embs_dir: str) -> Tuple[str, np.ndarray]:\n",
    "    # Extract base filename without extension\n",
    "    base = os.path.splitext(os.path.basename(txt_path))[0]\n",
    "    # Construct corresponding pickle path\n",
    "    pkl_path = os.path.join(embs_dir, base + \".pkl\")\n",
    "    # Ensure the embedding file exists\n",
    "    if not os.path.exists(pkl_path):\n",
    "        raise FileNotFoundError(f\"No embedding .pkl for {txt_path}\")\n",
    "    # Read the chunk text\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    # Load the embedding vector from pickle\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        emb = pickle.load(f)\n",
    "    # Convert the embedding to a NumPy array\n",
    "    emb_array = np.array(emb, dtype=\"float32\")\n",
    "    return text, emb_array  # Return chunk text and its embedding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "821d0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the stored chunks and their corresponding embeddings back into memory\n",
    "\n",
    "def load_local_embeddings(chunks_dir: str, embs_dir: str, max_workers: int = 8) -> Tuple[List[str], np.ndarray]:\n",
    "    # Find all text chunk files in the directory\n",
    "    txt_files = sorted(glob.glob(os.path.join(chunks_dir, \"chunk_*.txt\"))) # Get all chunk text files, sorted\n",
    "    if not txt_files:\n",
    "        raise ValueError(f\"No chunk_*.txt files found in {chunks_dir!r}\")  # Raise error if no chunk files found\n",
    "    \n",
    "    texts: List[str] = []      # List to store loaded chunk texts\n",
    "    embs: List[np.ndarray] = []  # List to store loaded embeddings\n",
    "\n",
    "    # Load data in parallel using ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit a loading task for each chunk file\n",
    "        futures = {executor.submit(_load_one_pair, txt, embs_dir): txt for txt in txt_files}\n",
    "        for fut in as_completed(futures): # As each task completes\n",
    "            src = futures[fut] # Get the source file for this future\n",
    "            try:\n",
    "                text, emb = fut.result()  # Get the loaded text and embedding\n",
    "                texts.append(text)        # Add text to list\n",
    "                embs.append(emb)          # Add embedding to list\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] loading {src!r}: {e}\")\n",
    "\n",
    "    if not embs:\n",
    "        raise ValueError(\"No embeddings loaded!\")\n",
    "\n",
    "    # Combine all embeddings into a single NumPy array (matrix)\n",
    "    embs_array = np.vstack(embs)\n",
    "    return texts, embs_array  # Return list of texts and embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7df30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 2. Build a FAISS index ────────────────────────────────────\n",
    "def build_faiss_index(embs: np.ndarray) -> faiss.IndexFlatIP:\n",
    "    # Get the dimensionality of the embeddings\n",
    "    dim = embs.shape[1]\n",
    "    # Create an index for Inner Product (cosine similarity)\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    # Normalize the embeddings to unit vectors for cosine similarity\n",
    "    faiss.normalize_L2(embs)\n",
    "    # Add all embeddings to the index\n",
    "    index.add(embs)\n",
    "    return index  # Return FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e569df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 3. Embed a query via Azure OpenAI ─────────────────────────\n",
    "def get_query_embedding(query: str, embeddings_model: AzureOpenAIEmbeddings) -> np.ndarray:\n",
    "    # Get query embedding using Azure OpenAI model\n",
    "    emb = embeddings_model.embed_query(query)\n",
    "    # Convert to float32 NumPy array\n",
    "    arr = np.array(emb, dtype=\"float32\")\n",
    "    # Normalize the embedding\n",
    "    arr /= np.linalg.norm(arr)\n",
    "    return arr.reshape(1, -1)  # Return 2D array for FAISS query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c691944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LLM to validate & respond based on top FAISS chunks\n",
    "def query_with_llm(query: str, texts: List[str], index: faiss.IndexFlatIP,\n",
    "                   embeddings_model: AzureOpenAIEmbeddings, llm, top_k: int = 5) -> str:\n",
    "    # 1. Embed the user query\n",
    "    q_emb = get_query_embedding(query, embeddings_model)\n",
    "\n",
    "    # 2. Retrieve top-k similar chunks from FAISS\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "    retrieved_chunks = [texts[i] for i in I[0]]\n",
    "\n",
    "    # 3. Prepare context string for LLM\n",
    "    context = \"\\n\\n\".join(f\"Chunk #{i+1}:\\n{texts[i]}\" for i in I[0])\n",
    "\n",
    "    # 4. Compose the final prompt\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant. Given the following context chunks from a document and a user query, answer the query accurately using only the context provided.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Query:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    # 5. Send prompt to LLM\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    return response.content.strip()  # Clean and return LLM's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37199a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Main PDF Split & Local FAISS Flow ───────────────────────────\n",
    "def split_pdf_semantically(\n",
    "    pdf_path: str,\n",
    "    chunks_output_dir: str,\n",
    "    embeddings_model: AzureOpenAIEmbeddings = None,\n",
    "    embeddings_output_dir: str = \"\",\n",
    "    post_index_query: Optional[str] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "      1) Extract and chunk PDF\n",
    "      2) Persist chunks as .txt\n",
    "      3) Generate & save embeddings (.pkl)\n",
    "      4) Load local embeddings + FAISS + LLM\n",
    "    \"\"\"\n",
    "    # 1–3: extract, detect, chunk, save\n",
    "    print(\"🔍 Extracting layout-aware text blocks...\")\n",
    "    blocks = extract_blocks_with_fonts(pdf_path)\n",
    "    print(\"🧠 Detecting semantic headings...\")\n",
    "    sections = detect_headings(blocks)\n",
    "    print(\"📦 Chunking content...\")\n",
    "    chunks = chunk_sections(sections)\n",
    "    print(f\"💾 Saving chunks to {chunks_output_dir}\")\n",
    "    save_chunks(chunks, chunks_output_dir)\n",
    "\n",
    "    print(f\"🤖 Generating embeddings for {len(chunks)} chunks...\")\n",
    "    embeddings = embed_chunks_parallel(chunks, embeddings_model)\n",
    "    print(f\"💾 Saving embeddings to {embeddings_output_dir}\")\n",
    "    save_embeddings(embeddings, embeddings_output_dir)\n",
    "\n",
    "    # 4: local FAISS flow\n",
    "    print(\"🗄️ Loading local embeddings and building FAISS index...\")\n",
    "    texts, embs_matrix = load_local_embeddings(chunks_output_dir, embeddings_output_dir)\n",
    "    faiss_index = build_faiss_index(embs_matrix)\n",
    "    if post_index_query:\n",
    "        print(f\"🔎 Running local FAISS+LLM for: '{post_index_query}'\")\n",
    "        local_ans = query_with_llm(\n",
    "            post_index_query, texts, faiss_index,\n",
    "            embeddings_model, llm=llm, top_k=3\n",
    "        )\n",
    "        print(\"💬 Local FAISS+LLM Answer:\", local_ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e162d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Extracting layout-aware text blocks...\n",
      "🧠 Detecting semantic headings...\n",
      "📦 Chunking content...\n",
      "💾 Saving chunks to azureai_chunks\n",
      "🤖 Generating embeddings for 89 chunks...\n",
      "[chunk_20] Embedding generated.\n",
      "[chunk_13] Embedding generated.\n",
      "[chunk_15] Embedding generated.\n",
      "[chunk_3] Embedding generated.\n",
      "[chunk_6] Embedding generated.\n",
      "[chunk_18] Embedding generated.\n",
      "[chunk_19] Embedding generated.\n",
      "[chunk_14] Embedding generated.\n",
      "[chunk_21] Embedding generated.\n",
      "[chunk_17] Embedding generated.\n",
      "[chunk_27] Embedding generated.\n",
      "[chunk_5] Embedding generated.\n",
      "[chunk_26] Embedding generated.\n",
      "[chunk_2] Embedding generated.\n",
      "[chunk_4] Embedding generated.\n",
      "[chunk_16] Embedding generated.\n",
      "[chunk_11] Embedding generated.\n",
      "[chunk_23] Embedding generated.\n",
      "[chunk_7] Embedding generated.\n",
      "[chunk_10] Embedding generated.\n",
      "[chunk_24] Embedding generated.\n",
      "[chunk_8] Embedding generated.\n",
      "[chunk_22] Embedding generated.\n",
      "[chunk_9] Embedding generated.\n",
      "[chunk_1] Embedding generated.\n",
      "[chunk_28] Embedding generated.\n",
      "[chunk_12] Embedding generated.\n",
      "[chunk_25] Embedding generated.\n",
      "[chunk_30] Embedding generated.\n",
      "[chunk_31] Embedding generated.\n",
      "[chunk_29] Embedding generated.\n",
      "[chunk_36] Embedding generated.\n",
      "[chunk_41] Embedding generated.\n",
      "[chunk_35] Embedding generated.\n",
      "[chunk_42] Embedding generated.\n",
      "[chunk_34] Embedding generated.\n",
      "[chunk_43] Embedding generated.\n",
      "[chunk_38] Embedding generated.\n",
      "[chunk_37] Embedding generated.\n",
      "[chunk_44] Embedding generated.\n",
      "[chunk_40] Embedding generated.\n",
      "[chunk_39] Embedding generated.\n",
      "[chunk_46] Embedding generated.\n",
      "[chunk_33] Embedding generated.\n",
      "[chunk_49] Embedding generated.\n",
      "[chunk_48] Embedding generated.\n",
      "[chunk_32] Embedding generated.\n",
      "[chunk_50] Embedding generated.\n",
      "[chunk_51] Embedding generated.\n",
      "[chunk_54] Embedding generated.\n",
      "[chunk_45] Embedding generated.\n",
      "[chunk_52] Embedding generated.\n",
      "[chunk_55] Embedding generated.\n",
      "[chunk_53] Embedding generated.\n",
      "[chunk_47] Embedding generated.\n",
      "[chunk_56] Embedding generated.\n",
      "[chunk_62] Embedding generated.\n",
      "[chunk_59] Embedding generated.\n",
      "[chunk_60] Embedding generated.\n",
      "[chunk_58] Embedding generated.\n",
      "[chunk_63] Embedding generated.\n",
      "[chunk_57] Embedding generated.\n",
      "[chunk_61] Embedding generated.\n",
      "[chunk_65] Embedding generated.\n",
      "[chunk_66] Embedding generated.\n",
      "[chunk_64] Embedding generated.\n",
      "[chunk_69] Embedding generated.\n",
      "[chunk_68] Embedding generated.\n",
      "[chunk_70] Embedding generated.\n",
      "[chunk_76] Embedding generated.\n",
      "[chunk_73] Embedding generated.\n",
      "[chunk_71] Embedding generated.\n",
      "[chunk_67] Embedding generated.\n",
      "[chunk_72] Embedding generated.\n",
      "[chunk_74] Embedding generated.\n",
      "[chunk_75] Embedding generated.\n",
      "[chunk_80] Embedding generated.\n",
      "[chunk_89] Embedding generated.\n",
      "[chunk_78] Embedding generated.\n",
      "[chunk_84] Embedding generated.\n",
      "[chunk_85] Embedding generated.\n",
      "[chunk_86] Embedding generated.\n",
      "[chunk_79] Embedding generated.\n",
      "[chunk_77] Embedding generated.\n",
      "[chunk_82] Embedding generated.\n",
      "[chunk_88] Embedding generated.\n",
      "[chunk_81] Embedding generated.\n",
      "[chunk_83] Embedding generated.\n",
      "[chunk_87] Embedding generated.\n",
      "💾 Saving embeddings to azureai_embeddings_folder\n",
      "[chunk_20] Embedding saved to: azureai_embeddings_folder\\chunk_20.pkl\n",
      "[chunk_13] Embedding saved to: azureai_embeddings_folder\\chunk_13.pkl\n",
      "[chunk_15] Embedding saved to: azureai_embeddings_folder\\chunk_15.pkl\n",
      "[chunk_3] Embedding saved to: azureai_embeddings_folder\\chunk_3.pkl\n",
      "[chunk_6] Embedding saved to: azureai_embeddings_folder\\chunk_6.pkl\n",
      "[chunk_18] Embedding saved to: azureai_embeddings_folder\\chunk_18.pkl\n",
      "[chunk_19] Embedding saved to: azureai_embeddings_folder\\chunk_19.pkl\n",
      "[chunk_14] Embedding saved to: azureai_embeddings_folder\\chunk_14.pkl\n",
      "[chunk_21] Embedding saved to: azureai_embeddings_folder\\chunk_21.pkl\n",
      "[chunk_17] Embedding saved to: azureai_embeddings_folder\\chunk_17.pkl\n",
      "[chunk_27] Embedding saved to: azureai_embeddings_folder\\chunk_27.pkl\n",
      "[chunk_5] Embedding saved to: azureai_embeddings_folder\\chunk_5.pkl\n",
      "[chunk_26] Embedding saved to: azureai_embeddings_folder\\chunk_26.pkl\n",
      "[chunk_2] Embedding saved to: azureai_embeddings_folder\\chunk_2.pkl\n",
      "[chunk_4] Embedding saved to: azureai_embeddings_folder\\chunk_4.pkl\n",
      "[chunk_16] Embedding saved to: azureai_embeddings_folder\\chunk_16.pkl\n",
      "[chunk_11] Embedding saved to: azureai_embeddings_folder\\chunk_11.pkl\n",
      "[chunk_23] Embedding saved to: azureai_embeddings_folder\\chunk_23.pkl\n",
      "[chunk_7] Embedding saved to: azureai_embeddings_folder\\chunk_7.pkl\n",
      "[chunk_10] Embedding saved to: azureai_embeddings_folder\\chunk_10.pkl\n",
      "[chunk_24] Embedding saved to: azureai_embeddings_folder\\chunk_24.pkl\n",
      "[chunk_8] Embedding saved to: azureai_embeddings_folder\\chunk_8.pkl\n",
      "[chunk_22] Embedding saved to: azureai_embeddings_folder\\chunk_22.pkl\n",
      "[chunk_9] Embedding saved to: azureai_embeddings_folder\\chunk_9.pkl\n",
      "[chunk_1] Embedding saved to: azureai_embeddings_folder\\chunk_1.pkl\n",
      "[chunk_28] Embedding saved to: azureai_embeddings_folder\\chunk_28.pkl\n",
      "[chunk_12] Embedding saved to: azureai_embeddings_folder\\chunk_12.pkl\n",
      "[chunk_25] Embedding saved to: azureai_embeddings_folder\\chunk_25.pkl\n",
      "[chunk_30] Embedding saved to: azureai_embeddings_folder\\chunk_30.pkl\n",
      "[chunk_31] Embedding saved to: azureai_embeddings_folder\\chunk_31.pkl\n",
      "[chunk_29] Embedding saved to: azureai_embeddings_folder\\chunk_29.pkl\n",
      "[chunk_36] Embedding saved to: azureai_embeddings_folder\\chunk_36.pkl\n",
      "[chunk_41] Embedding saved to: azureai_embeddings_folder\\chunk_41.pkl\n",
      "[chunk_35] Embedding saved to: azureai_embeddings_folder\\chunk_35.pkl\n",
      "[chunk_42] Embedding saved to: azureai_embeddings_folder\\chunk_42.pkl\n",
      "[chunk_34] Embedding saved to: azureai_embeddings_folder\\chunk_34.pkl\n",
      "[chunk_43] Embedding saved to: azureai_embeddings_folder\\chunk_43.pkl\n",
      "[chunk_38] Embedding saved to: azureai_embeddings_folder\\chunk_38.pkl\n",
      "[chunk_37] Embedding saved to: azureai_embeddings_folder\\chunk_37.pkl\n",
      "[chunk_44] Embedding saved to: azureai_embeddings_folder\\chunk_44.pkl\n",
      "[chunk_40] Embedding saved to: azureai_embeddings_folder\\chunk_40.pkl\n",
      "[chunk_39] Embedding saved to: azureai_embeddings_folder\\chunk_39.pkl\n",
      "[chunk_46] Embedding saved to: azureai_embeddings_folder\\chunk_46.pkl\n",
      "[chunk_33] Embedding saved to: azureai_embeddings_folder\\chunk_33.pkl\n",
      "[chunk_49] Embedding saved to: azureai_embeddings_folder\\chunk_49.pkl\n",
      "[chunk_48] Embedding saved to: azureai_embeddings_folder\\chunk_48.pkl\n",
      "[chunk_32] Embedding saved to: azureai_embeddings_folder\\chunk_32.pkl\n",
      "[chunk_50] Embedding saved to: azureai_embeddings_folder\\chunk_50.pkl\n",
      "[chunk_51] Embedding saved to: azureai_embeddings_folder\\chunk_51.pkl\n",
      "[chunk_54] Embedding saved to: azureai_embeddings_folder\\chunk_54.pkl\n",
      "[chunk_45] Embedding saved to: azureai_embeddings_folder\\chunk_45.pkl\n",
      "[chunk_52] Embedding saved to: azureai_embeddings_folder\\chunk_52.pkl\n",
      "[chunk_55] Embedding saved to: azureai_embeddings_folder\\chunk_55.pkl\n",
      "[chunk_53] Embedding saved to: azureai_embeddings_folder\\chunk_53.pkl\n",
      "[chunk_47] Embedding saved to: azureai_embeddings_folder\\chunk_47.pkl\n",
      "[chunk_56] Embedding saved to: azureai_embeddings_folder\\chunk_56.pkl\n",
      "[chunk_62] Embedding saved to: azureai_embeddings_folder\\chunk_62.pkl\n",
      "[chunk_59] Embedding saved to: azureai_embeddings_folder\\chunk_59.pkl\n",
      "[chunk_60] Embedding saved to: azureai_embeddings_folder\\chunk_60.pkl\n",
      "[chunk_58] Embedding saved to: azureai_embeddings_folder\\chunk_58.pkl\n",
      "[chunk_63] Embedding saved to: azureai_embeddings_folder\\chunk_63.pkl\n",
      "[chunk_57] Embedding saved to: azureai_embeddings_folder\\chunk_57.pkl\n",
      "[chunk_61] Embedding saved to: azureai_embeddings_folder\\chunk_61.pkl\n",
      "[chunk_65] Embedding saved to: azureai_embeddings_folder\\chunk_65.pkl\n",
      "[chunk_66] Embedding saved to: azureai_embeddings_folder\\chunk_66.pkl\n",
      "[chunk_64] Embedding saved to: azureai_embeddings_folder\\chunk_64.pkl\n",
      "[chunk_69] Embedding saved to: azureai_embeddings_folder\\chunk_69.pkl\n",
      "[chunk_68] Embedding saved to: azureai_embeddings_folder\\chunk_68.pkl\n",
      "[chunk_70] Embedding saved to: azureai_embeddings_folder\\chunk_70.pkl\n",
      "[chunk_76] Embedding saved to: azureai_embeddings_folder\\chunk_76.pkl\n",
      "[chunk_73] Embedding saved to: azureai_embeddings_folder\\chunk_73.pkl\n",
      "[chunk_71] Embedding saved to: azureai_embeddings_folder\\chunk_71.pkl\n",
      "[chunk_67] Embedding saved to: azureai_embeddings_folder\\chunk_67.pkl\n",
      "[chunk_72] Embedding saved to: azureai_embeddings_folder\\chunk_72.pkl\n",
      "[chunk_74] Embedding saved to: azureai_embeddings_folder\\chunk_74.pkl\n",
      "[chunk_75] Embedding saved to: azureai_embeddings_folder\\chunk_75.pkl\n",
      "[chunk_80] Embedding saved to: azureai_embeddings_folder\\chunk_80.pkl\n",
      "[chunk_89] Embedding saved to: azureai_embeddings_folder\\chunk_89.pkl\n",
      "[chunk_78] Embedding saved to: azureai_embeddings_folder\\chunk_78.pkl\n",
      "[chunk_84] Embedding saved to: azureai_embeddings_folder\\chunk_84.pkl\n",
      "[chunk_85] Embedding saved to: azureai_embeddings_folder\\chunk_85.pkl\n",
      "[chunk_86] Embedding saved to: azureai_embeddings_folder\\chunk_86.pkl\n",
      "[chunk_79] Embedding saved to: azureai_embeddings_folder\\chunk_79.pkl\n",
      "[chunk_77] Embedding saved to: azureai_embeddings_folder\\chunk_77.pkl\n",
      "[chunk_82] Embedding saved to: azureai_embeddings_folder\\chunk_82.pkl\n",
      "[chunk_88] Embedding saved to: azureai_embeddings_folder\\chunk_88.pkl\n",
      "[chunk_81] Embedding saved to: azureai_embeddings_folder\\chunk_81.pkl\n",
      "[chunk_83] Embedding saved to: azureai_embeddings_folder\\chunk_83.pkl\n",
      "[chunk_87] Embedding saved to: azureai_embeddings_folder\\chunk_87.pkl\n",
      "🗄️ Loading local embeddings and building FAISS index...\n",
      "🔎 Running local FAISS+LLM for: 'What is Anesthesia, general?'\n",
      "💬 Local FAISS+LLM Answer: General anesthesia is the induction of a state of unconsciousness with the absence of pain sensation over the entire body, achieved through the administration of anesthetic drugs. It is utilized during certain medical and surgical procedures for various purposes, including pain relief (analgesia), blocking memory of the procedure (amnesia), producing unconsciousness, inhibiting normal body reflexes to make surgery safe and easier to perform, and relaxing the muscles of the body. General anesthesia involves different stages and can be administered using gases or volatile liquids vaporized and inhaled with oxygen, or delivered intravenously. The combination of inhaled and intravenous agents is termed balanced anesthesia. This practice takes advantage of the beneficial effects of each anesthetic agent to reach surgical anesthesia. General anesthesia operates by altering the flow of sodium molecules into neurons, which prevents nerve impulses and leads to unconsciousness, lack of memory, pain perception, and control of involuntary reflexes during procedures.\n"
     ]
    }
   ],
   "source": [
    "split_pdf_semantically(\n",
    "        pdf_path=r\"#your_pdf_file_path#\",\n",
    "        chunks_output_dir=\"#your_chunks_output_dir#\",\n",
    "        embeddings_model=embedding_model,\n",
    "        embeddings_output_dir=\"#your_embeddings_output_dir#\",\n",
    "        post_index_query=\"#your_query_here#\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
